{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFDE Datapackage Merge\n",
    "\n",
    "## How to use\n",
    "1. Download the latest preparation script as well as the external vocabulary table\n",
    "2. `pip install -r requirements.txt`\n",
    "3. edit inputdir (cell 2) and outdir (cell 15, 16, 17) \n",
    "4. run all cells\n",
    "5. `cfde-submit run <directory_to_submit (outdir)> --output-dir=<new_output_directory> --dcc idg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from datapackage import DataPackage\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from c2m2_frictionless import C2M2, create_datapackage, validate_datapackage, validate_id_namespace_name_uniqueness, build_term_tables\n",
    "import time\n",
    "import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdir = \"06152022/\"\n",
    "namespace = \"https://www.druggablegenome.net/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_id_namespace():\n",
    "    for inputpath in glob.glob(inputdir + \"data_*\"):\n",
    "        id_namespace = pd.read_csv(inputpath + \"/id_namespace.tsv\", sep=\"\\t\")\n",
    "        id_namespace = id_namespace.fillna(\"\")\n",
    "        for i in id_namespace.index:\n",
    "            yield C2M2.id_namespace(\n",
    "                id=id_namespace.at[i,\"id\"],\n",
    "                abbreviation=id_namespace.at[i,\"abbreviation\"],\n",
    "                name=id_namespace.at[i,\"name\"],\n",
    "                description=id_namespace.at[i,\"description\"],\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_dcc():\n",
    "    yield C2M2.dcc(\n",
    "        id=\"cfde_registry_dcc:idg\",\n",
    "        dcc_name=\"Illuminating the Druggable Genome\",\n",
    "        dcc_abbreviation=\"IDG\",\n",
    "        dcc_description=\"The goal of IDG is to improve our understanding of understudied proteins from the three most commonly drug-targeted protein families: G-protein coupled receptors, ion channels, and protein kinases.\",\n",
    "        contact_email=\"JJYang@salud.unm.edu\",\n",
    "        contact_name=\"Jeremy Yang\",\n",
    "        dcc_url=\"https://www.druggablegenome.net/\",\n",
    "        project_id_namespace=namespace,\n",
    "        project_local_id=\"IDG\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_project():\n",
    "    for inputpath in glob.glob(inputdir + \"data_*\"):\n",
    "        project = pd.read_csv(inputpath + \"/project.tsv\", sep=\"\\t\")\n",
    "        project = project.fillna(\"\")\n",
    "        for i in project.index:\n",
    "            yield C2M2.project(\n",
    "                id_namespace=project.at[i, \"id_namespace\"],\n",
    "                local_id=project.at[i, \"local_id\"],\n",
    "                persistent_id=project.at[i, \"persistent_id\"],\n",
    "                creation_time=project.at[i, \"creation_time\"],\n",
    "                abbreviation=project.at[i, \"abbreviation\"],\n",
    "                name=project.at[i, \"name\"],\n",
    "                description=project.at[i, \"description\"],\n",
    "            )\n",
    "            if not project.at[i, \"local_id\"] == \"IDG\":\n",
    "                yield C2M2.project_in_project(\n",
    "                    parent_project_id_namespace=namespace,\n",
    "                    parent_project_local_id=\"IDG\",\n",
    "                    child_project_id_namespace=project.at[i, \"id_namespace\"],\n",
    "                    child_project_local_id=project.at[i, \"local_id\"]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_file():\n",
    "    for inputpath in glob.glob(inputdir + \"data_*\"):\n",
    "        file = pd.read_csv(inputpath + \"/file.tsv\", sep=\"\\t\")\n",
    "        for i in file.index:\n",
    "            local_id=file.at[i, \"local_id\"]\n",
    "            yield C2M2.file(\n",
    "                id_namespace=file.at[i, \"id_namespace\"],\n",
    "                local_id=local_id ,\n",
    "                project_id_namespace=file.at[i, \"project_id_namespace\"],\n",
    "                project_local_id=file.at[i, \"project_local_id\"],\n",
    "                persistent_id=file.at[i, \"persistent_id\"],\n",
    "                creation_time=file.at[i, \"creation_time\"],\n",
    "                size_in_bytes=int(file.at[i, \"size_in_bytes\"]) if not not file.at[i, \"size_in_bytes\"] == \"\" and not pd.isna(file.at[i, \"size_in_bytes\"]) else \"\",\n",
    "                uncompressed_size_in_bytes=int(file.at[i, \"uncompressed_size_in_bytes\"]) if not file.at[i, \"uncompressed_size_in_bytes\"] == \"\" and not pd.isna(file.at[i, \"uncompressed_size_in_bytes\"]) else \"\",\n",
    "                sha256=file.at[i, \"sha256\"],\n",
    "                md5=file.at[i, \"md5\"],\n",
    "                filename=file.at[i, \"filename\"],\n",
    "                file_format=file.at[i, \"file_format\"] if not pd.isna(file.at[i, \"file_format\"]) else \"\",\n",
    "                data_type=file.at[i, \"data_type\"] if not pd.isna(file.at[i, \"data_type\"]) else \"\",\n",
    "                assay_type=file.at[i, \"assay_type\"] if not pd.isna(file.at[i, \"assay_type\"]) else \"\",\n",
    "                analysis_type=file.at[i, \"analysis_type\"] if not pd.isna(file.at[i, \"analysis_type\"]) else \"\",\n",
    "                mime_type=file.at[i, \"mime_type\"],\n",
    "                compression_format=file.at[i, \"compression_format\"] if local_id.endswith(\".gz\") else \"\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_file_in_collection():\n",
    "    for inputpath in glob.glob(inputdir + \"data_*\"):\n",
    "        file = pd.read_csv(inputpath + \"/file_in_collection.tsv\", sep=\"\\t\")\n",
    "        f = pd.read_csv(inputpath + \"/file.tsv\", sep=\"\\t\", index_col=1)\n",
    "        for i in file.index:\n",
    "            file_local_id = file.at[i, \"file_local_id\"]\n",
    "            yield C2M2.file_in_collection(\n",
    "                file_id_namespace=file.at[i, \"file_id_namespace\"],\n",
    "                file_local_id=file_local_id,\n",
    "                collection_id_namespace=file.at[i, \"collection_id_namespace\"],\n",
    "                collection_local_id=file.at[i, \"collection_local_id\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_collection_compound():\n",
    "    for inputpath in glob.glob(inputdir + \"data_*\"):\n",
    "        file = pd.read_csv(inputpath + \"/collection_compound.tsv\", sep=\"\\t\")\n",
    "        for i in file.index:\n",
    "            yield C2M2.collection_compound(\n",
    "                collection_id_namespace=file.at[i, \"collection_id_namespace\"],\n",
    "                collection_local_id=file.at[i, \"collection_local_id\"],\n",
    "                compound=file.at[i, \"compound\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_collection_defined_by_project():\n",
    "    for inputpath in glob.glob(inputdir + \"data_*\"):\n",
    "        file = pd.read_csv(inputpath + \"/collection_defined_by_project.tsv\", sep=\"\\t\")\n",
    "        for i in file.index:\n",
    "            yield C2M2.collection_defined_by_project(\n",
    "                collection_id_namespace=file.at[i, \"collection_id_namespace\"],\n",
    "                collection_local_id=file.at[i, \"collection_local_id\"],\n",
    "                project_id_namespace=file.at[i, \"project_id_namespace\"],\n",
    "                project_local_id=file.at[i, \"project_local_id\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_collection_disease():\n",
    "    for inputpath in glob.glob(inputdir + \"data_*\"):\n",
    "        file = pd.read_csv(inputpath + \"/collection_disease.tsv\", sep=\"\\t\")\n",
    "        for i in file.index:\n",
    "            yield C2M2.collection_disease(\n",
    "                collection_id_namespace=file.at[i, \"collection_id_namespace\"],\n",
    "                collection_local_id=file.at[i, \"collection_local_id\"],\n",
    "                disease=file.at[i, \"disease\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_collection_gene():\n",
    "    for inputpath in glob.glob(inputdir + \"data_*\"):\n",
    "        file = pd.read_csv(inputpath + \"/collection_gene.tsv\", sep=\"\\t\")\n",
    "        for i in file.index:\n",
    "            yield C2M2.collection_gene(\n",
    "                collection_id_namespace=file.at[i, \"collection_id_namespace\"],\n",
    "                collection_local_id=file.at[i, \"collection_local_id\"],\n",
    "                gene=file.at[i, \"gene\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_collection_taxonomy():\n",
    "    for inputpath in glob.glob(inputdir + \"data_*\"):\n",
    "        file = pd.read_csv(inputpath + \"/collection_taxonomy.tsv\", sep=\"\\t\")\n",
    "        for i in file.index:\n",
    "            yield C2M2.collection_taxonomy(\n",
    "                collection_id_namespace=file.at[i, \"collection_id_namespace\"],\n",
    "                collection_local_id=file.at[i, \"collection_local_id\"],\n",
    "                taxon=file.at[i, \"taxon\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_collection():\n",
    "    for inputpath in glob.glob(inputdir + \"data_*\"):\n",
    "        file = pd.read_csv(inputpath + \"/collection.tsv\", sep=\"\\t\")\n",
    "        for i in file.index:\n",
    "            persistent_id = file.at[i, \"persistent_id\"]\n",
    "            yield C2M2.collection(\n",
    "                id_namespace=file.at[i, \"id_namespace\"],\n",
    "                local_id=file.at[i, \"local_id\"],\n",
    "                persistent_id=file.at[i, \"persistent_id\"].replace(\"_associations_associations\", \"_associations\"),\n",
    "                creation_time=file.at[i, \"creation_time\"] if not pd.isna(file.at[i, \"creation_time\"]) else \"\",\n",
    "                abbreviation=file.at[i, \"abbreviation\"] if not pd.isna(file.at[i, \"abbreviation\"]) else \"\",\n",
    "                name=file.at[i, \"name\"] if not pd.isna(file.at[i, \"name\"]) else \"\",\n",
    "                description=file.at[i, \"description\"] if not pd.isna(file.at[i, \"description\"]) else \"\",\n",
    "                has_time_series_data=file.at[i, \"has_time_series_data\"] if not pd.isna(file.at[i, \"has_time_series_data\"]) else \"\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " def convert_idg_to_c2m2():\n",
    "    for i in yield_id_namespace():\n",
    "        yield i\n",
    "    for i in yield_dcc():\n",
    "        yield i\n",
    "    for i in yield_project():\n",
    "        yield i\n",
    "    for i in yield_file():\n",
    "        yield i\n",
    "    for i in yield_file_in_collection():\n",
    "        yield i\n",
    "    for i in yield_collection_compound():\n",
    "        yield i\n",
    "    for i in yield_collection_defined_by_project():\n",
    "        yield i\n",
    "    for i in yield_collection_disease():\n",
    "        yield i\n",
    "    for i in yield_collection_gene():\n",
    "        yield i\n",
    "    for i in yield_collection_taxonomy():\n",
    "        yield i\n",
    "    for i in yield_collection():\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema found 06152022/C2M2_datapackage.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jp/qvftth6s7r3fxqkrdr7vyq1c0000gn/T/ipykernel_64068/487716186.py:3: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file = pd.read_csv(inputpath + \"/file.tsv\", sep=\"\\t\")\n",
      "/var/folders/jp/qvftth6s7r3fxqkrdr7vyq1c0000gn/T/ipykernel_64068/1229405774.py:4: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  f = pd.read_csv(inputpath + \"/file.tsv\", sep=\"\\t\", index_col=1)\n"
     ]
    }
   ],
   "source": [
    "outdir=\"06152022/IDG_merged\"\n",
    "pkg = create_datapackage('C2M2', convert_idg_to_c2m2(), outdir, schema_file=\"06152022/C2M2_datapackage.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all CV terms used in submission tables... [Wed Jun 15 21:10:52 EDT 2022]\n",
      "\n",
      "   scanning \"06152022/IDG_merged/file.tsv\"...\n",
      "   scanning \"06152022/IDG_merged/biosample.tsv\"...\n",
      "   scanning \"06152022/IDG_merged/biosample_disease.tsv\"...\n",
      "   scanning \"06152022/IDG_merged/biosample_gene.tsv\"...\n",
      "   scanning \"06152022/IDG_merged/biosample_substance.tsv\"...\n",
      "   scanning \"06152022/IDG_merged/subject_disease.tsv\"...\n",
      "   scanning \"06152022/IDG_merged/subject_phenotype.tsv\"...\n",
      "   scanning \"06152022/IDG_merged/subject_role_taxonomy.tsv\"...\n",
      "   scanning \"06152022/IDG_merged/subject_substance.tsv\"...\n",
      "   scanning \"06152022/IDG_merged/collection_anatomy.tsv\"...\n",
      "   scanning \"06152022/IDG_merged/collection_compound.tsv\"...\n",
      "   scanning \"06152022/IDG_merged/collection_disease.tsv\"...\n",
      "   scanning \"06152022/IDG_merged/collection_gene.tsv\"...\n",
      "   scanning \"06152022/IDG_merged/collection_phenotype.tsv\"...\n",
      "   scanning \"06152022/IDG_merged/collection_protein.tsv\"...\n",
      "   scanning \"06152022/IDG_merged/collection_substance.tsv\"...\n",
      "   scanning \"06152022/IDG_merged/collection_taxonomy.tsv\"...\n",
      "\n",
      "...done scanning all CV terms used in this submission. [Wed Jun 15 21:10:53 EDT 2022]\n",
      "Loading display data (names, descriptions, etc.) for all CV terms used in submission tables... [Wed Jun 15 21:10:53 EDT 2022]\n",
      "\n",
      "   Human Phenotype Ontology... [Wed Jun 15 21:10:53 EDT 2022]\n",
      "   Uber-anatomy ontology... [Wed Jun 15 21:10:57 EDT 2022]\n",
      "   Ontology for Biomedical Investigations (assay type)... [Wed Jun 15 21:10:59 EDT 2022]\n",
      "   Ontology for Biomedical Investigations (analysis type)... [Wed Jun 15 21:10:59 EDT 2022]\n",
      "   EDAM (data type)... [Wed Jun 15 21:10:59 EDT 2022]\n",
      "   Disease Ontology... [Wed Jun 15 21:10:59 EDT 2022]\n",
      "   EDAM (file format)... [Wed Jun 15 21:11:00 EDT 2022]\n",
      "   Ensembl genes... [Wed Jun 15 21:11:00 EDT 2022]\n",
      "   UniProtKB proteins... [if you included protein terms in your submission, this step will take around 10 minutes, even with a fast disk] [Wed Jun 15 21:11:00 EDT 2022]\n",
      "   PubChem and GlyTouCan substances and compounds [if you included substances and/or compounds in your submission, this step will take around 10 minutes, even with a fast disk]...\n",
      "      ...processing compound file... [Wed Jun 15 21:11:00 EDT 2022]\n",
      "   NCBI Taxonomy... [Wed Jun 15 21:15:55 EDT 2022]\n",
      "\n",
      "...done loading display data for all CVs. [Wed Jun 15 21:16:00 EDT 2022]\n",
      "Writing CV term tracker tables (to be included in complete C2M2 submission)... [Wed Jun 15 21:16:00 EDT 2022]\n",
      "\n",
      "   writing \"autogenerated_C2M2_term_tables/anatomy.tsv\"... [Wed Jun 15 21:16:00 EDT 2022]\n",
      "   writing \"autogenerated_C2M2_term_tables/assay_type.tsv\"... [Wed Jun 15 21:16:00 EDT 2022]\n",
      "   writing \"autogenerated_C2M2_term_tables/analysis_type.tsv\"... [Wed Jun 15 21:16:00 EDT 2022]\n",
      "   writing \"autogenerated_C2M2_term_tables/compound.tsv\"... [Wed Jun 15 21:16:00 EDT 2022]\n",
      "   writing \"autogenerated_C2M2_term_tables/data_type.tsv\"... [Wed Jun 15 21:16:00 EDT 2022]\n",
      "   writing \"autogenerated_C2M2_term_tables/disease.tsv\"... [Wed Jun 15 21:16:00 EDT 2022]\n",
      "   writing \"autogenerated_C2M2_term_tables/file_format.tsv\"... [Wed Jun 15 21:16:00 EDT 2022]\n",
      "   writing \"autogenerated_C2M2_term_tables/gene.tsv\"... [Wed Jun 15 21:16:00 EDT 2022]\n",
      "   writing \"autogenerated_C2M2_term_tables/ncbi_taxonomy.tsv\"... [Wed Jun 15 21:16:00 EDT 2022]\n",
      "   writing \"autogenerated_C2M2_term_tables/phenotype.tsv\"... [Wed Jun 15 21:16:00 EDT 2022]\n",
      "   writing \"autogenerated_C2M2_term_tables/protein.tsv\"... [Wed Jun 15 21:16:00 EDT 2022]\n",
      "   writing \"autogenerated_C2M2_term_tables/substance.tsv\"... [Wed Jun 15 21:16:00 EDT 2022]\n",
      "\n",
      "...done writing term tables for final C2M2 submission datapackage. [Wed Jun 15 21:16:00 EDT 2022]\n",
      "Ensuring all file.tsv records with persistent IDs have non-null checksums... [Wed Jun 15 21:16:00 EDT 2022]\n",
      "\n",
      "...done; all file records passed checksum verification. [Wed Jun 15 21:16:01 EDT 2022]\n",
      "Ensuring all persistent IDs are unique (both within and across tables)... [Wed Jun 15 21:16:01 EDT 2022]\n",
      "\n",
      "...done; all persistent IDs verified to be unique. [Wed Jun 15 21:16:01 EDT 2022]\n"
     ]
    }
   ],
   "source": [
    "!python3 preparation_scripts/v12.py 06152022/IDG_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp autogenerated_C2M2_term_tables/* 06152022/IDG_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file...\n",
      "Success (file).\n",
      "Checking biosample...\n",
      "Success (biosample).\n",
      "Checking subject...\n",
      "Success (subject).\n",
      "Checking dcc...\n",
      "Success (dcc).\n",
      "Checking project...\n",
      "Success (project).\n",
      "Checking project_in_project...\n",
      "Success (project_in_project).\n",
      "Checking collection...\n",
      "Success (collection).\n",
      "Checking collection_in_collection...\n",
      "Success (collection_in_collection).\n",
      "Checking file_describes_collection...\n",
      "Success (file_describes_collection).\n",
      "Checking collection_defined_by_project...\n",
      "Success (collection_defined_by_project).\n",
      "Checking file_in_collection...\n",
      "Success (file_in_collection).\n",
      "Checking biosample_in_collection...\n",
      "Success (biosample_in_collection).\n",
      "Checking subject_in_collection...\n",
      "Success (subject_in_collection).\n",
      "Checking file_describes_biosample...\n",
      "Success (file_describes_biosample).\n",
      "Checking file_describes_subject...\n",
      "Success (file_describes_subject).\n",
      "Checking biosample_from_subject...\n",
      "Success (biosample_from_subject).\n",
      "Checking biosample_disease...\n",
      "Success (biosample_disease).\n",
      "Checking subject_disease...\n",
      "Success (subject_disease).\n",
      "Checking collection_disease...\n",
      "Success (collection_disease).\n",
      "Checking collection_phenotype...\n",
      "Success (collection_phenotype).\n",
      "Checking collection_gene...\n",
      "Success (collection_gene).\n",
      "Checking collection_compound...\n",
      "Success (collection_compound).\n",
      "Checking collection_substance...\n",
      "Success (collection_substance).\n",
      "Checking collection_taxonomy...\n",
      "Success (collection_taxonomy).\n",
      "Checking collection_anatomy...\n",
      "Success (collection_anatomy).\n",
      "Checking collection_protein...\n",
      "Success (collection_protein).\n",
      "Checking subject_phenotype...\n",
      "Success (subject_phenotype).\n",
      "Checking biosample_substance...\n",
      "Success (biosample_substance).\n",
      "Checking subject_substance...\n",
      "Success (subject_substance).\n",
      "Checking biosample_gene...\n",
      "Success (biosample_gene).\n",
      "Checking phenotype_gene...\n",
      "Success (phenotype_gene).\n",
      "Checking phenotype_disease...\n",
      "Success (phenotype_disease).\n",
      "Checking subject_race...\n",
      "Success (subject_race).\n",
      "Checking subject_role_taxonomy...\n",
      "Success (subject_role_taxonomy).\n",
      "Checking assay_type...\n",
      "Success (assay_type).\n",
      "Checking analysis_type...\n",
      "Success (analysis_type).\n",
      "Checking ncbi_taxonomy...\n",
      "Success (ncbi_taxonomy).\n",
      "Checking anatomy...\n",
      "Success (anatomy).\n",
      "Checking file_format...\n",
      "Success (file_format).\n",
      "Checking data_type...\n",
      "Success (data_type).\n",
      "Checking disease...\n",
      "Success (disease).\n",
      "Checking phenotype...\n",
      "Success (phenotype).\n",
      "Checking compound...\n",
      "Success (compound).\n",
      "Checking substance...\n",
      "Success (substance).\n",
      "Checking gene...\n",
      "Success (gene).\n",
      "Checking protein...\n",
      "Success (protein).\n",
      "Checking protein_gene...\n",
      "Success (protein_gene).\n",
      "Checking id_namespace...\n",
      "Success (id_namespace).\n"
     ]
    }
   ],
   "source": [
    "validate_datapackage(pkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idg-venv",
   "language": "python",
   "name": "idg-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
